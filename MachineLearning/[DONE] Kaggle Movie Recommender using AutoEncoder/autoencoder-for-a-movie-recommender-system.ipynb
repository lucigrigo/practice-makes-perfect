{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017312,
     "end_time": "2020-10-31T07:56:42.792940",
     "exception": false,
     "start_time": "2020-10-31T07:56:42.775628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " \n",
    "# This notebook comes as a helping hand to anyone interested in building or learning how to build a **movie recommender system using AutoEncoders**! Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015832,
     "end_time": "2020-10-31T07:56:42.824955",
     "exception": false,
     "start_time": "2020-10-31T07:56:42.809123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are going to use \n",
    "* NumPy\n",
    "* Pandas\n",
    "\n",
    "and **PyTorch** for building the SAE (Sparse AutoEncoder)\n",
    "    More information about Sparse Autoencoders can be found [here](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017355,
     "end_time": "2020-10-31T07:56:42.858530",
     "exception": false,
     "start_time": "2020-10-31T07:56:42.841175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:42.898281Z",
     "iopub.status.busy": "2020-10-31T07:56:42.897462Z",
     "iopub.status.idle": "2020-10-31T07:56:44.096997Z",
     "shell.execute_reply": "2020-10-31T07:56:44.096031Z"
    },
    "papermill": {
     "duration": 1.222515,
     "end_time": "2020-10-31T07:56:44.097187",
     "exception": false,
     "start_time": "2020-10-31T07:56:42.874672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016116,
     "end_time": "2020-10-31T07:56:44.132713",
     "exception": false,
     "start_time": "2020-10-31T07:56:44.116597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:44.173951Z",
     "iopub.status.busy": "2020-10-31T07:56:44.173107Z",
     "iopub.status.idle": "2020-10-31T07:56:53.573070Z",
     "shell.execute_reply": "2020-10-31T07:56:53.572175Z"
    },
    "papermill": {
     "duration": 9.424191,
     "end_time": "2020-10-31T07:56:53.573213",
     "exception": false,
     "start_time": "2020-10-31T07:56:44.149022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "movies = pd.read_csv('/kaggle/input/movie-recommender-dataset/ml-1m/ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('/kaggle/input/movie-recommender-dataset/ml-1m/ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = pd.read_csv('/kaggle/input/movie-recommender-dataset/ml-1m/ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016141,
     "end_time": "2020-10-31T07:56:53.606222",
     "exception": false,
     "start_time": "2020-10-31T07:56:53.590081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:53.653491Z",
     "iopub.status.busy": "2020-10-31T07:56:53.652511Z",
     "iopub.status.idle": "2020-10-31T07:56:53.715739Z",
     "shell.execute_reply": "2020-10-31T07:56:53.716326Z"
    },
    "papermill": {
     "duration": 0.093536,
     "end_time": "2020-10-31T07:56:53.716532",
     "exception": false,
     "start_time": "2020-10-31T07:56:53.622996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating train and test sets\n",
    "training_set = pd.read_csv('/kaggle/input/movie-recommender-dataset/ml-100k/ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('/kaggle/input/movie-recommender-dataset/ml-100k/ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016446,
     "end_time": "2020-10-31T07:56:53.749935",
     "exception": false,
     "start_time": "2020-10-31T07:56:53.733489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Getting number of users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:53.844897Z",
     "iopub.status.busy": "2020-10-31T07:56:53.817195Z",
     "iopub.status.idle": "2020-10-31T07:56:53.848908Z",
     "shell.execute_reply": "2020-10-31T07:56:53.849614Z"
    },
    "papermill": {
     "duration": 0.083282,
     "end_time": "2020-10-31T07:56:53.849802",
     "exception": false,
     "start_time": "2020-10-31T07:56:53.766520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of users = 943\n",
      "no of movies = 1682\n"
     ]
    }
   ],
   "source": [
    "# getting the number of users and movies\n",
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n",
    "print('no of users = ' + str(nb_users))\n",
    "print('no of movies = ' + str(nb_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017591,
     "end_time": "2020-10-31T07:56:53.885325",
     "exception": false,
     "start_time": "2020-10-31T07:56:53.867734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Converting the data into an array with users as lines and movies as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:53.930080Z",
     "iopub.status.busy": "2020-10-31T07:56:53.929180Z",
     "iopub.status.idle": "2020-10-31T07:56:54.453400Z",
     "shell.execute_reply": "2020-10-31T07:56:54.452592Z"
    },
    "papermill": {
     "duration": 0.550703,
     "end_time": "2020-10-31T07:56:54.453548",
     "exception": false,
     "start_time": "2020-10-31T07:56:53.902845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting the data into an array with users as lines and movies as columns\n",
    "def conv(data):\n",
    "    new_data = []\n",
    "    for id_user in range(1, nb_users + 1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "\n",
    "# converting test and train sets\n",
    "training_set = conv(training_set)\n",
    "test_set = conv(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017387,
     "end_time": "2020-10-31T07:56:54.488599",
     "exception": false,
     "start_time": "2020-10-31T07:56:54.471212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "More information about Torch tensors and how they work can be found [here](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01736,
     "end_time": "2020-10-31T07:56:54.523468",
     "exception": false,
     "start_time": "2020-10-31T07:56:54.506108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Converting the data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:54.609640Z",
     "iopub.status.busy": "2020-10-31T07:56:54.608726Z",
     "iopub.status.idle": "2020-10-31T07:56:55.089810Z",
     "shell.execute_reply": "2020-10-31T07:56:55.088885Z"
    },
    "papermill": {
     "duration": 0.548784,
     "end_time": "2020-10-31T07:56:55.089945",
     "exception": false,
     "start_time": "2020-10-31T07:56:54.541161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017891,
     "end_time": "2020-10-31T07:56:55.125743",
     "exception": false,
     "start_time": "2020-10-31T07:56:55.107852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "More information about the **torch.nn** module can be found [here](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017491,
     "end_time": "2020-10-31T07:56:55.161295",
     "exception": false,
     "start_time": "2020-10-31T07:56:55.143804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating the arhitecture of the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:55.217319Z",
     "iopub.status.busy": "2020-10-31T07:56:55.216550Z",
     "iopub.status.idle": "2020-10-31T07:56:55.237600Z",
     "shell.execute_reply": "2020-10-31T07:56:55.236801Z"
    },
    "papermill": {
     "duration": 0.058656,
     "end_time": "2020-10-31T07:56:55.237735",
     "exception": false,
     "start_time": "2020-10-31T07:56:55.179079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating the architecture of the Neural Network (SAE in our case is considered a directed neural network)\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018159,
     "end_time": "2020-10-31T07:56:55.276663",
     "exception": false,
     "start_time": "2020-10-31T07:56:55.258504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-31T07:56:55.333984Z",
     "iopub.status.busy": "2020-10-31T07:56:55.323310Z",
     "iopub.status.idle": "2020-10-31T08:02:13.906314Z",
     "shell.execute_reply": "2020-10-31T08:02:13.907091Z"
    },
    "papermill": {
     "duration": 318.611541,
     "end_time": "2020-10-31T08:02:13.907301",
     "exception": false,
     "start_time": "2020-10-31T07:56:55.295760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_loss: tensor(1.7712)\n",
      "epoch: 2 train_loss: tensor(1.0968)\n",
      "epoch: 3 train_loss: tensor(1.0533)\n",
      "epoch: 4 train_loss: tensor(1.0381)\n",
      "epoch: 5 train_loss: tensor(1.0310)\n",
      "epoch: 6 train_loss: tensor(1.0265)\n",
      "epoch: 7 train_loss: tensor(1.0238)\n",
      "epoch: 8 train_loss: tensor(1.0220)\n",
      "epoch: 9 train_loss: tensor(1.0210)\n",
      "epoch: 10 train_loss: tensor(1.0194)\n",
      "epoch: 11 train_loss: tensor(1.0190)\n",
      "epoch: 12 train_loss: tensor(1.0183)\n",
      "epoch: 13 train_loss: tensor(1.0180)\n",
      "epoch: 14 train_loss: tensor(1.0174)\n",
      "epoch: 15 train_loss: tensor(1.0174)\n",
      "epoch: 16 train_loss: tensor(1.0167)\n",
      "epoch: 17 train_loss: tensor(1.0169)\n",
      "epoch: 18 train_loss: tensor(1.0164)\n",
      "epoch: 19 train_loss: tensor(1.0163)\n",
      "epoch: 20 train_loss: tensor(1.0161)\n",
      "epoch: 21 train_loss: tensor(1.0161)\n",
      "epoch: 22 train_loss: tensor(1.0160)\n",
      "epoch: 23 train_loss: tensor(1.0159)\n",
      "epoch: 24 train_loss: tensor(1.0159)\n",
      "epoch: 25 train_loss: tensor(1.0158)\n",
      "epoch: 26 train_loss: tensor(1.0155)\n",
      "epoch: 27 train_loss: tensor(1.0153)\n",
      "epoch: 28 train_loss: tensor(1.0151)\n",
      "epoch: 29 train_loss: tensor(1.0131)\n",
      "epoch: 30 train_loss: tensor(1.0116)\n",
      "epoch: 31 train_loss: tensor(1.0113)\n",
      "epoch: 32 train_loss: tensor(1.0090)\n",
      "epoch: 33 train_loss: tensor(1.0089)\n",
      "epoch: 34 train_loss: tensor(1.0044)\n",
      "epoch: 35 train_loss: tensor(1.0050)\n",
      "epoch: 36 train_loss: tensor(1.0013)\n",
      "epoch: 37 train_loss: tensor(1.0009)\n",
      "epoch: 38 train_loss: tensor(0.9967)\n",
      "epoch: 39 train_loss: tensor(0.9957)\n",
      "epoch: 40 train_loss: tensor(0.9943)\n",
      "epoch: 41 train_loss: tensor(0.9962)\n",
      "epoch: 42 train_loss: tensor(0.9921)\n",
      "epoch: 43 train_loss: tensor(0.9917)\n",
      "epoch: 44 train_loss: tensor(0.9881)\n",
      "epoch: 45 train_loss: tensor(0.9886)\n",
      "epoch: 46 train_loss: tensor(0.9837)\n",
      "epoch: 47 train_loss: tensor(0.9846)\n",
      "epoch: 48 train_loss: tensor(0.9796)\n",
      "epoch: 49 train_loss: tensor(0.9809)\n",
      "epoch: 50 train_loss: tensor(0.9794)\n",
      "epoch: 51 train_loss: tensor(0.9791)\n",
      "epoch: 52 train_loss: tensor(0.9760)\n",
      "epoch: 53 train_loss: tensor(0.9744)\n",
      "epoch: 54 train_loss: tensor(0.9690)\n",
      "epoch: 55 train_loss: tensor(0.9657)\n",
      "epoch: 56 train_loss: tensor(0.9649)\n",
      "epoch: 57 train_loss: tensor(0.9656)\n",
      "epoch: 58 train_loss: tensor(0.9648)\n",
      "epoch: 59 train_loss: tensor(0.9639)\n",
      "epoch: 60 train_loss: tensor(0.9594)\n",
      "epoch: 61 train_loss: tensor(0.9612)\n",
      "epoch: 62 train_loss: tensor(0.9615)\n",
      "epoch: 63 train_loss: tensor(0.9599)\n",
      "epoch: 64 train_loss: tensor(0.9572)\n",
      "epoch: 65 train_loss: tensor(0.9590)\n",
      "epoch: 66 train_loss: tensor(0.9571)\n",
      "epoch: 67 train_loss: tensor(0.9560)\n",
      "epoch: 68 train_loss: tensor(0.9547)\n",
      "epoch: 69 train_loss: tensor(0.9542)\n",
      "epoch: 70 train_loss: tensor(0.9538)\n",
      "epoch: 71 train_loss: tensor(0.9544)\n",
      "epoch: 72 train_loss: tensor(0.9514)\n",
      "epoch: 73 train_loss: tensor(0.9507)\n",
      "epoch: 74 train_loss: tensor(0.9488)\n",
      "epoch: 75 train_loss: tensor(0.9491)\n",
      "epoch: 76 train_loss: tensor(0.9465)\n",
      "epoch: 77 train_loss: tensor(0.9476)\n",
      "epoch: 78 train_loss: tensor(0.9445)\n",
      "epoch: 79 train_loss: tensor(0.9463)\n",
      "epoch: 80 train_loss: tensor(0.9432)\n",
      "epoch: 81 train_loss: tensor(0.9451)\n",
      "epoch: 82 train_loss: tensor(0.9429)\n",
      "epoch: 83 train_loss: tensor(0.9457)\n",
      "epoch: 84 train_loss: tensor(0.9423)\n",
      "epoch: 85 train_loss: tensor(0.9428)\n",
      "epoch: 86 train_loss: tensor(0.9411)\n",
      "epoch: 87 train_loss: tensor(0.9426)\n",
      "epoch: 88 train_loss: tensor(0.9403)\n",
      "epoch: 89 train_loss: tensor(0.9414)\n",
      "epoch: 90 train_loss: tensor(0.9397)\n",
      "epoch: 91 train_loss: tensor(0.9406)\n",
      "epoch: 92 train_loss: tensor(0.9393)\n",
      "epoch: 93 train_loss: tensor(0.9394)\n",
      "epoch: 94 train_loss: tensor(0.9381)\n",
      "epoch: 95 train_loss: tensor(0.9389)\n",
      "epoch: 96 train_loss: tensor(0.9370)\n",
      "epoch: 97 train_loss: tensor(0.9373)\n",
      "epoch: 98 train_loss: tensor(0.9364)\n",
      "epoch: 99 train_loss: tensor(0.9364)\n",
      "epoch: 100 train_loss: tensor(0.9353)\n",
      "epoch: 101 train_loss: tensor(0.9354)\n",
      "epoch: 102 train_loss: tensor(0.9349)\n",
      "epoch: 103 train_loss: tensor(0.9351)\n",
      "epoch: 104 train_loss: tensor(0.9340)\n",
      "epoch: 105 train_loss: tensor(0.9340)\n",
      "epoch: 106 train_loss: tensor(0.9334)\n",
      "epoch: 107 train_loss: tensor(0.9332)\n",
      "epoch: 108 train_loss: tensor(0.9326)\n",
      "epoch: 109 train_loss: tensor(0.9326)\n",
      "epoch: 110 train_loss: tensor(0.9321)\n",
      "epoch: 111 train_loss: tensor(0.9320)\n",
      "epoch: 112 train_loss: tensor(0.9318)\n",
      "epoch: 113 train_loss: tensor(0.9316)\n",
      "epoch: 114 train_loss: tensor(0.9309)\n",
      "epoch: 115 train_loss: tensor(0.9310)\n",
      "epoch: 116 train_loss: tensor(0.9303)\n",
      "epoch: 117 train_loss: tensor(0.9304)\n",
      "epoch: 118 train_loss: tensor(0.9298)\n",
      "epoch: 119 train_loss: tensor(0.9295)\n",
      "epoch: 120 train_loss: tensor(0.9292)\n",
      "epoch: 121 train_loss: tensor(0.9292)\n",
      "epoch: 122 train_loss: tensor(0.9288)\n",
      "epoch: 123 train_loss: tensor(0.9283)\n",
      "epoch: 124 train_loss: tensor(0.9282)\n",
      "epoch: 125 train_loss: tensor(0.9276)\n",
      "epoch: 126 train_loss: tensor(0.9277)\n",
      "epoch: 127 train_loss: tensor(0.9272)\n",
      "epoch: 128 train_loss: tensor(0.9271)\n",
      "epoch: 129 train_loss: tensor(0.9268)\n",
      "epoch: 130 train_loss: tensor(0.9268)\n",
      "epoch: 131 train_loss: tensor(0.9264)\n",
      "epoch: 132 train_loss: tensor(0.9262)\n",
      "epoch: 133 train_loss: tensor(0.9256)\n",
      "epoch: 134 train_loss: tensor(0.9257)\n",
      "epoch: 135 train_loss: tensor(0.9255)\n",
      "epoch: 136 train_loss: tensor(0.9257)\n",
      "epoch: 137 train_loss: tensor(0.9249)\n",
      "epoch: 138 train_loss: tensor(0.9246)\n",
      "epoch: 139 train_loss: tensor(0.9243)\n",
      "epoch: 140 train_loss: tensor(0.9243)\n",
      "epoch: 141 train_loss: tensor(0.9244)\n",
      "epoch: 142 train_loss: tensor(0.9240)\n",
      "epoch: 143 train_loss: tensor(0.9236)\n",
      "epoch: 144 train_loss: tensor(0.9236)\n",
      "epoch: 145 train_loss: tensor(0.9233)\n",
      "epoch: 146 train_loss: tensor(0.9232)\n",
      "epoch: 147 train_loss: tensor(0.9227)\n",
      "epoch: 148 train_loss: tensor(0.9225)\n",
      "epoch: 149 train_loss: tensor(0.9222)\n",
      "epoch: 150 train_loss: tensor(0.9221)\n",
      "epoch: 151 train_loss: tensor(0.9222)\n",
      "epoch: 152 train_loss: tensor(0.9217)\n",
      "epoch: 153 train_loss: tensor(0.9214)\n",
      "epoch: 154 train_loss: tensor(0.9210)\n",
      "epoch: 155 train_loss: tensor(0.9210)\n",
      "epoch: 156 train_loss: tensor(0.9207)\n",
      "epoch: 157 train_loss: tensor(0.9205)\n",
      "epoch: 158 train_loss: tensor(0.9202)\n",
      "epoch: 159 train_loss: tensor(0.9202)\n",
      "epoch: 160 train_loss: tensor(0.9199)\n",
      "epoch: 161 train_loss: tensor(0.9200)\n",
      "epoch: 162 train_loss: tensor(0.9196)\n",
      "epoch: 163 train_loss: tensor(0.9195)\n",
      "epoch: 164 train_loss: tensor(0.9193)\n",
      "epoch: 165 train_loss: tensor(0.9192)\n",
      "epoch: 166 train_loss: tensor(0.9188)\n",
      "epoch: 167 train_loss: tensor(0.9188)\n",
      "epoch: 168 train_loss: tensor(0.9185)\n",
      "epoch: 169 train_loss: tensor(0.9185)\n",
      "epoch: 170 train_loss: tensor(0.9182)\n",
      "epoch: 171 train_loss: tensor(0.9185)\n",
      "epoch: 172 train_loss: tensor(0.9178)\n",
      "epoch: 173 train_loss: tensor(0.9175)\n",
      "epoch: 174 train_loss: tensor(0.9174)\n",
      "epoch: 175 train_loss: tensor(0.9178)\n",
      "epoch: 176 train_loss: tensor(0.9172)\n",
      "epoch: 177 train_loss: tensor(0.9170)\n",
      "epoch: 178 train_loss: tensor(0.9167)\n",
      "epoch: 179 train_loss: tensor(0.9168)\n",
      "epoch: 180 train_loss: tensor(0.9163)\n",
      "epoch: 181 train_loss: tensor(0.9157)\n",
      "epoch: 182 train_loss: tensor(0.9162)\n",
      "epoch: 183 train_loss: tensor(0.9164)\n",
      "epoch: 184 train_loss: tensor(0.9158)\n",
      "epoch: 185 train_loss: tensor(0.9161)\n",
      "epoch: 186 train_loss: tensor(0.9160)\n",
      "epoch: 187 train_loss: tensor(0.9160)\n",
      "epoch: 188 train_loss: tensor(0.9155)\n",
      "epoch: 189 train_loss: tensor(0.9156)\n",
      "epoch: 190 train_loss: tensor(0.9152)\n",
      "epoch: 191 train_loss: tensor(0.9158)\n",
      "epoch: 192 train_loss: tensor(0.9149)\n",
      "epoch: 193 train_loss: tensor(0.9152)\n",
      "epoch: 194 train_loss: tensor(0.9148)\n",
      "epoch: 195 train_loss: tensor(0.9151)\n",
      "epoch: 196 train_loss: tensor(0.9145)\n",
      "epoch: 197 train_loss: tensor(0.9148)\n",
      "epoch: 198 train_loss: tensor(0.9143)\n",
      "epoch: 199 train_loss: tensor(0.9146)\n",
      "epoch: 200 train_loss: tensor(0.9142)\n"
     ]
    }
   ],
   "source": [
    "# training the SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data * mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: ' + str(epoch) + ' train_loss: ' + str(train_loss / s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113483,
     "end_time": "2020-10-31T08:02:14.125008",
     "exception": false,
     "start_time": "2020-10-31T08:02:14.011525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-31T08:02:14.340671Z",
     "iopub.status.busy": "2020-10-31T08:02:14.339818Z",
     "iopub.status.idle": "2020-10-31T08:02:15.100401Z",
     "shell.execute_reply": "2020-10-31T08:02:15.099537Z"
    },
    "papermill": {
     "duration": 0.882852,
     "end_time": "2020-10-31T08:02:15.107496",
     "exception": false,
     "start_time": "2020-10-31T08:02:14.224644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1682])) that is different to the input size (torch.Size([1, 1682])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(1.0053)\n",
      "test loss: tensor(0.9441)\n",
      "test loss: tensor(1.0152)\n",
      "test loss: tensor(0.8887)\n",
      "test loss: tensor(0.9338)\n",
      "test loss: tensor(0.9305)\n",
      "test loss: tensor(0.9408)\n",
      "test loss: tensor(0.9373)\n",
      "test loss: tensor(0.9199)\n",
      "test loss: tensor(0.8978)\n",
      "test loss: tensor(0.8953)\n",
      "test loss: tensor(0.8867)\n",
      "test loss: tensor(0.9162)\n",
      "test loss: tensor(0.9302)\n",
      "test loss: tensor(0.9578)\n",
      "test loss: tensor(0.9586)\n",
      "test loss: tensor(0.9659)\n",
      "test loss: tensor(0.9548)\n",
      "test loss: tensor(0.9536)\n",
      "test loss: tensor(0.9708)\n",
      "test loss: tensor(0.9752)\n",
      "test loss: tensor(0.9841)\n",
      "test loss: tensor(0.9801)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9639)\n",
      "test loss: tensor(0.9451)\n",
      "test loss: tensor(0.9412)\n",
      "test loss: tensor(0.9358)\n",
      "test loss: tensor(0.9251)\n",
      "test loss: tensor(0.9182)\n",
      "test loss: tensor(0.9207)\n",
      "test loss: tensor(0.9170)\n",
      "test loss: tensor(0.9070)\n",
      "test loss: tensor(0.9025)\n",
      "test loss: tensor(0.9073)\n",
      "test loss: tensor(0.9224)\n",
      "test loss: tensor(0.9216)\n",
      "test loss: tensor(0.9447)\n",
      "test loss: tensor(0.9501)\n",
      "test loss: tensor(0.9517)\n",
      "test loss: tensor(0.9461)\n",
      "test loss: tensor(0.9465)\n",
      "test loss: tensor(0.9458)\n",
      "test loss: tensor(0.9468)\n",
      "test loss: tensor(0.9421)\n",
      "test loss: tensor(0.9405)\n",
      "test loss: tensor(0.9369)\n",
      "test loss: tensor(0.9365)\n",
      "test loss: tensor(0.9387)\n",
      "test loss: tensor(0.9447)\n",
      "test loss: tensor(0.9467)\n",
      "test loss: tensor(0.9403)\n",
      "test loss: tensor(0.9413)\n",
      "test loss: tensor(0.9457)\n",
      "test loss: tensor(0.9571)\n",
      "test loss: tensor(0.9556)\n",
      "test loss: tensor(0.9530)\n",
      "test loss: tensor(0.9554)\n",
      "test loss: tensor(0.9576)\n",
      "test loss: tensor(0.9523)\n",
      "test loss: tensor(0.9582)\n",
      "test loss: tensor(0.9598)\n",
      "test loss: tensor(0.9603)\n",
      "test loss: tensor(0.9589)\n",
      "test loss: tensor(0.9603)\n",
      "test loss: tensor(0.9622)\n",
      "test loss: tensor(0.9601)\n",
      "test loss: tensor(0.9665)\n",
      "test loss: tensor(0.9660)\n",
      "test loss: tensor(0.9608)\n",
      "test loss: tensor(0.9634)\n",
      "test loss: tensor(0.9623)\n",
      "test loss: tensor(0.9614)\n",
      "test loss: tensor(0.9594)\n",
      "test loss: tensor(0.9646)\n",
      "test loss: tensor(0.9654)\n",
      "test loss: tensor(0.9669)\n",
      "test loss: tensor(0.9707)\n",
      "test loss: tensor(0.9718)\n",
      "test loss: tensor(0.9725)\n",
      "test loss: tensor(0.9749)\n",
      "test loss: tensor(0.9740)\n",
      "test loss: tensor(0.9735)\n",
      "test loss: tensor(0.9691)\n",
      "test loss: tensor(0.9658)\n",
      "test loss: tensor(0.9651)\n",
      "test loss: tensor(0.9631)\n",
      "test loss: tensor(0.9682)\n",
      "test loss: tensor(0.9706)\n",
      "test loss: tensor(0.9685)\n",
      "test loss: tensor(0.9682)\n",
      "test loss: tensor(0.9660)\n",
      "test loss: tensor(0.9697)\n",
      "test loss: tensor(0.9682)\n",
      "test loss: tensor(0.9688)\n",
      "test loss: tensor(0.9697)\n",
      "test loss: tensor(0.9719)\n",
      "test loss: tensor(0.9758)\n",
      "test loss: tensor(0.9768)\n",
      "test loss: tensor(0.9768)\n",
      "test loss: tensor(0.9755)\n",
      "test loss: tensor(0.9724)\n",
      "test loss: tensor(0.9694)\n",
      "test loss: tensor(0.9689)\n",
      "test loss: tensor(0.9675)\n",
      "test loss: tensor(0.9646)\n",
      "test loss: tensor(0.9661)\n",
      "test loss: tensor(0.9652)\n",
      "test loss: tensor(0.9671)\n",
      "test loss: tensor(0.9658)\n",
      "test loss: tensor(0.9646)\n",
      "test loss: tensor(0.9686)\n",
      "test loss: tensor(0.9717)\n",
      "test loss: tensor(0.9734)\n",
      "test loss: tensor(0.9758)\n",
      "test loss: tensor(0.9743)\n",
      "test loss: tensor(0.9735)\n",
      "test loss: tensor(0.9701)\n",
      "test loss: tensor(0.9696)\n",
      "test loss: tensor(0.9693)\n",
      "test loss: tensor(0.9705)\n",
      "test loss: tensor(0.9705)\n",
      "test loss: tensor(0.9706)\n",
      "test loss: tensor(0.9741)\n",
      "test loss: tensor(0.9756)\n",
      "test loss: tensor(0.9774)\n",
      "test loss: tensor(0.9822)\n",
      "test loss: tensor(0.9816)\n",
      "test loss: tensor(0.9821)\n",
      "test loss: tensor(0.9818)\n",
      "test loss: tensor(0.9809)\n",
      "test loss: tensor(0.9797)\n",
      "test loss: tensor(0.9783)\n",
      "test loss: tensor(0.9783)\n",
      "test loss: tensor(0.9758)\n",
      "test loss: tensor(0.9754)\n",
      "test loss: tensor(0.9761)\n",
      "test loss: tensor(0.9743)\n",
      "test loss: tensor(0.9713)\n",
      "test loss: tensor(0.9694)\n",
      "test loss: tensor(0.9723)\n",
      "test loss: tensor(0.9716)\n",
      "test loss: tensor(0.9729)\n",
      "test loss: tensor(0.9720)\n",
      "test loss: tensor(0.9731)\n",
      "test loss: tensor(0.9716)\n",
      "test loss: tensor(0.9704)\n",
      "test loss: tensor(0.9722)\n",
      "test loss: tensor(0.9723)\n",
      "test loss: tensor(0.9723)\n",
      "test loss: tensor(0.9718)\n",
      "test loss: tensor(0.9717)\n",
      "test loss: tensor(0.9731)\n",
      "test loss: tensor(0.9721)\n",
      "test loss: tensor(0.9702)\n",
      "test loss: tensor(0.9693)\n",
      "test loss: tensor(0.9707)\n",
      "test loss: tensor(0.9692)\n",
      "test loss: tensor(0.9704)\n",
      "test loss: tensor(0.9708)\n",
      "test loss: tensor(0.9718)\n",
      "test loss: tensor(0.9712)\n",
      "test loss: tensor(0.9686)\n",
      "test loss: tensor(0.9680)\n",
      "test loss: tensor(0.9676)\n",
      "test loss: tensor(0.9716)\n",
      "test loss: tensor(0.9765)\n",
      "test loss: tensor(0.9784)\n",
      "test loss: tensor(0.9777)\n",
      "test loss: tensor(0.9774)\n",
      "test loss: tensor(0.9744)\n",
      "test loss: tensor(0.9734)\n",
      "test loss: tensor(0.9717)\n",
      "test loss: tensor(0.9740)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9737)\n",
      "test loss: tensor(0.9735)\n",
      "test loss: tensor(0.9726)\n",
      "test loss: tensor(0.9754)\n",
      "test loss: tensor(0.9759)\n",
      "test loss: tensor(0.9754)\n",
      "test loss: tensor(0.9752)\n",
      "test loss: tensor(0.9752)\n",
      "test loss: tensor(0.9737)\n",
      "test loss: tensor(0.9719)\n",
      "test loss: tensor(0.9731)\n",
      "test loss: tensor(0.9731)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9722)\n",
      "test loss: tensor(0.9720)\n",
      "test loss: tensor(0.9725)\n",
      "test loss: tensor(0.9731)\n",
      "test loss: tensor(0.9722)\n",
      "test loss: tensor(0.9714)\n",
      "test loss: tensor(0.9725)\n",
      "test loss: tensor(0.9730)\n",
      "test loss: tensor(0.9720)\n",
      "test loss: tensor(0.9729)\n",
      "test loss: tensor(0.9719)\n",
      "test loss: tensor(0.9719)\n",
      "test loss: tensor(0.9744)\n",
      "test loss: tensor(0.9741)\n",
      "test loss: tensor(0.9754)\n",
      "test loss: tensor(0.9750)\n",
      "test loss: tensor(0.9769)\n",
      "test loss: tensor(0.9764)\n",
      "test loss: tensor(0.9770)\n",
      "test loss: tensor(0.9768)\n",
      "test loss: tensor(0.9757)\n",
      "test loss: tensor(0.9768)\n",
      "test loss: tensor(0.9805)\n",
      "test loss: tensor(0.9799)\n",
      "test loss: tensor(0.9792)\n",
      "test loss: tensor(0.9788)\n",
      "test loss: tensor(0.9782)\n",
      "test loss: tensor(0.9799)\n",
      "test loss: tensor(0.9789)\n",
      "test loss: tensor(0.9824)\n",
      "test loss: tensor(0.9822)\n",
      "test loss: tensor(0.9818)\n",
      "test loss: tensor(0.9818)\n",
      "test loss: tensor(0.9832)\n",
      "test loss: tensor(0.9838)\n",
      "test loss: tensor(0.9841)\n",
      "test loss: tensor(0.9833)\n",
      "test loss: tensor(0.9830)\n",
      "test loss: tensor(0.9846)\n",
      "test loss: tensor(0.9868)\n",
      "test loss: tensor(0.9876)\n",
      "test loss: tensor(0.9887)\n",
      "test loss: tensor(0.9884)\n",
      "test loss: tensor(0.9875)\n",
      "test loss: tensor(0.9875)\n",
      "test loss: tensor(0.9865)\n",
      "test loss: tensor(0.9874)\n",
      "test loss: tensor(0.9862)\n",
      "test loss: tensor(0.9848)\n",
      "test loss: tensor(0.9852)\n",
      "test loss: tensor(0.9854)\n",
      "test loss: tensor(0.9853)\n",
      "test loss: tensor(0.9866)\n",
      "test loss: tensor(0.9864)\n",
      "test loss: tensor(0.9866)\n",
      "test loss: tensor(0.9873)\n",
      "test loss: tensor(0.9877)\n",
      "test loss: tensor(0.9869)\n",
      "test loss: tensor(0.9877)\n",
      "test loss: tensor(0.9868)\n",
      "test loss: tensor(0.9872)\n",
      "test loss: tensor(0.9867)\n",
      "test loss: tensor(0.9859)\n",
      "test loss: tensor(0.9853)\n",
      "test loss: tensor(0.9856)\n",
      "test loss: tensor(0.9860)\n",
      "test loss: tensor(0.9858)\n",
      "test loss: tensor(0.9854)\n",
      "test loss: tensor(0.9864)\n",
      "test loss: tensor(0.9852)\n",
      "test loss: tensor(0.9872)\n",
      "test loss: tensor(0.9874)\n",
      "test loss: tensor(0.9866)\n",
      "test loss: tensor(0.9862)\n",
      "test loss: tensor(0.9860)\n",
      "test loss: tensor(0.9864)\n",
      "test loss: tensor(0.9848)\n",
      "test loss: tensor(0.9833)\n",
      "test loss: tensor(0.9828)\n",
      "test loss: tensor(0.9839)\n",
      "test loss: tensor(0.9843)\n",
      "test loss: tensor(0.9836)\n",
      "test loss: tensor(0.9823)\n",
      "test loss: tensor(0.9814)\n",
      "test loss: tensor(0.9808)\n",
      "test loss: tensor(0.9792)\n",
      "test loss: tensor(0.9783)\n",
      "test loss: tensor(0.9772)\n",
      "test loss: tensor(0.9762)\n",
      "test loss: tensor(0.9774)\n",
      "test loss: tensor(0.9777)\n",
      "test loss: tensor(0.9776)\n",
      "test loss: tensor(0.9777)\n",
      "test loss: tensor(0.9768)\n",
      "test loss: tensor(0.9753)\n",
      "test loss: tensor(0.9752)\n",
      "test loss: tensor(0.9764)\n",
      "test loss: tensor(0.9762)\n",
      "test loss: tensor(0.9761)\n",
      "test loss: tensor(0.9764)\n",
      "test loss: tensor(0.9761)\n",
      "test loss: tensor(0.9760)\n",
      "test loss: tensor(0.9751)\n",
      "test loss: tensor(0.9745)\n",
      "test loss: tensor(0.9738)\n",
      "test loss: tensor(0.9733)\n",
      "test loss: tensor(0.9738)\n",
      "test loss: tensor(0.9736)\n",
      "test loss: tensor(0.9730)\n",
      "test loss: tensor(0.9723)\n",
      "test loss: tensor(0.9721)\n",
      "test loss: tensor(0.9723)\n",
      "test loss: tensor(0.9723)\n",
      "test loss: tensor(0.9723)\n",
      "test loss: tensor(0.9719)\n",
      "test loss: tensor(0.9724)\n",
      "test loss: tensor(0.9718)\n",
      "test loss: tensor(0.9716)\n",
      "test loss: tensor(0.9711)\n",
      "test loss: tensor(0.9724)\n",
      "test loss: tensor(0.9736)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9735)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9737)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9747)\n",
      "test loss: tensor(0.9748)\n",
      "test loss: tensor(0.9749)\n",
      "test loss: tensor(0.9747)\n",
      "test loss: tensor(0.9741)\n",
      "test loss: tensor(0.9734)\n",
      "test loss: tensor(0.9731)\n",
      "test loss: tensor(0.9727)\n",
      "test loss: tensor(0.9732)\n",
      "test loss: tensor(0.9734)\n",
      "test loss: tensor(0.9737)\n",
      "test loss: tensor(0.9735)\n",
      "test loss: tensor(0.9729)\n",
      "test loss: tensor(0.9719)\n",
      "test loss: tensor(0.9710)\n",
      "test loss: tensor(0.9711)\n",
      "test loss: tensor(0.9705)\n",
      "test loss: tensor(0.9719)\n",
      "test loss: tensor(0.9714)\n",
      "test loss: tensor(0.9714)\n",
      "test loss: tensor(0.9722)\n",
      "test loss: tensor(0.9717)\n",
      "test loss: tensor(0.9713)\n",
      "test loss: tensor(0.9707)\n",
      "test loss: tensor(0.9710)\n",
      "test loss: tensor(0.9710)\n",
      "test loss: tensor(0.9716)\n",
      "test loss: tensor(0.9713)\n",
      "test loss: tensor(0.9706)\n",
      "test loss: tensor(0.9703)\n",
      "test loss: tensor(0.9701)\n",
      "test loss: tensor(0.9710)\n",
      "test loss: tensor(0.9700)\n",
      "test loss: tensor(0.9700)\n",
      "test loss: tensor(0.9695)\n",
      "test loss: tensor(0.9699)\n",
      "test loss: tensor(0.9709)\n",
      "test loss: tensor(0.9711)\n",
      "test loss: tensor(0.9708)\n",
      "test loss: tensor(0.9696)\n",
      "test loss: tensor(0.9704)\n",
      "test loss: tensor(0.9696)\n",
      "test loss: tensor(0.9691)\n",
      "test loss: tensor(0.9686)\n",
      "test loss: tensor(0.9678)\n",
      "test loss: tensor(0.9673)\n",
      "test loss: tensor(0.9677)\n",
      "test loss: tensor(0.9680)\n",
      "test loss: tensor(0.9680)\n",
      "test loss: tensor(0.9681)\n",
      "test loss: tensor(0.9685)\n",
      "test loss: tensor(0.9675)\n",
      "test loss: tensor(0.9676)\n",
      "test loss: tensor(0.9667)\n",
      "test loss: tensor(0.9666)\n",
      "test loss: tensor(0.9670)\n",
      "test loss: tensor(0.9667)\n",
      "test loss: tensor(0.9667)\n",
      "test loss: tensor(0.9675)\n",
      "test loss: tensor(0.9674)\n",
      "test loss: tensor(0.9669)\n",
      "test loss: tensor(0.9664)\n",
      "test loss: tensor(0.9657)\n",
      "test loss: tensor(0.9654)\n",
      "test loss: tensor(0.9648)\n",
      "test loss: tensor(0.9653)\n",
      "test loss: tensor(0.9661)\n",
      "test loss: tensor(0.9664)\n",
      "test loss: tensor(0.9653)\n",
      "test loss: tensor(0.9661)\n",
      "test loss: tensor(0.9665)\n",
      "test loss: tensor(0.9667)\n",
      "test loss: tensor(0.9662)\n",
      "test loss: tensor(0.9657)\n",
      "test loss: tensor(0.9659)\n",
      "test loss: tensor(0.9659)\n",
      "test loss: tensor(0.9663)\n",
      "test loss: tensor(0.9664)\n",
      "test loss: tensor(0.9659)\n",
      "test loss: tensor(0.9653)\n",
      "test loss: tensor(0.9643)\n",
      "test loss: tensor(0.9644)\n",
      "test loss: tensor(0.9642)\n",
      "test loss: tensor(0.9643)\n",
      "test loss: tensor(0.9628)\n",
      "test loss: tensor(0.9632)\n",
      "test loss: tensor(0.9621)\n",
      "test loss: tensor(0.9615)\n",
      "test loss: tensor(0.9617)\n",
      "test loss: tensor(0.9624)\n",
      "test loss: tensor(0.9624)\n",
      "test loss: tensor(0.9618)\n",
      "test loss: tensor(0.9616)\n",
      "test loss: tensor(0.9622)\n",
      "test loss: tensor(0.9613)\n",
      "test loss: tensor(0.9607)\n",
      "test loss: tensor(0.9597)\n",
      "test loss: tensor(0.9594)\n",
      "test loss: tensor(0.9595)\n",
      "test loss: tensor(0.9596)\n",
      "test loss: tensor(0.9595)\n",
      "test loss: tensor(0.9591)\n",
      "test loss: tensor(0.9592)\n",
      "test loss: tensor(0.9584)\n",
      "test loss: tensor(0.9582)\n",
      "test loss: tensor(0.9575)\n",
      "test loss: tensor(0.9573)\n",
      "test loss: tensor(0.9565)\n",
      "test loss: tensor(0.9569)\n",
      "test loss: tensor(0.9577)\n",
      "test loss: tensor(0.9572)\n",
      "test loss: tensor(0.9571)\n",
      "test loss: tensor(0.9573)\n",
      "test loss: tensor(0.9567)\n",
      "test loss: tensor(0.9559)\n",
      "test loss: tensor(0.9545)\n",
      "test loss: tensor(0.9540)\n",
      "test loss: tensor(0.9533)\n",
      "test loss: tensor(0.9525)\n",
      "test loss: tensor(0.9523)\n",
      "test loss: tensor(0.9528)\n",
      "test loss: tensor(0.9529)\n",
      "test loss: tensor(0.9540)\n",
      "test loss: tensor(0.9537)\n",
      "test loss: tensor(0.9535)\n",
      "test loss: tensor(0.9535)\n",
      "test loss: tensor(0.9525)\n",
      "test loss: tensor(0.9527)\n",
      "test loss: tensor(0.9508)\n",
      "test loss: tensor(0.9503)\n",
      "test loss: tensor(0.9515)\n",
      "test loss: tensor(0.9512)\n",
      "test loss: tensor(0.9510)\n",
      "test loss: tensor(0.9526)\n",
      "test loss: tensor(0.9532)\n",
      "test loss: tensor(0.9533)\n",
      "test loss: tensor(0.9531)\n",
      "test loss: tensor(0.9523)\n",
      "test loss: tensor(0.9526)\n",
      "test loss: tensor(0.9514)\n",
      "test loss: tensor(0.9496)\n",
      "test loss: tensor(0.9476)\n",
      "test loss: tensor(0.9472)\n",
      "test loss: tensor(0.9487)\n"
     ]
    }
   ],
   "source": [
    "# testing the SAE\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[(target == 0).unsqueeze(0)] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data * mean_corrector)\n",
    "        s += 1.\n",
    "        print('test loss: '+ str(test_loss / s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.103248,
     "end_time": "2020-10-31T08:02:15.317856",
     "exception": false,
     "start_time": "2020-10-31T08:02:15.214608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The final loss on the test set is ~0.95 which proves to be an incredible result for our system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100697,
     "end_time": "2020-10-31T08:02:15.522222",
     "exception": false,
     "start_time": "2020-10-31T08:02:15.421525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*Thank you for your attention! :)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 337.679905,
   "end_time": "2020-10-31T08:02:15.734421",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-31T07:56:38.054516",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
