{"cells":[{"metadata":{"_uuid":"a177ef48-0045-4dae-ab23-46a9aff908ad","_cell_guid":"8f7f298e-d74e-412d-a665-450c9d07cd1c","trusted":true},"cell_type":"markdown","source":"_Hello and welcome to my notebook!\\\nFeel free to leave any feedback or observation in the comment section_"},{"metadata":{"_uuid":"f0a98a97-068a-48a2-b34b-318e6da3bc3d","_cell_guid":"96d46ce7-8498-4504-8ed6-9b2a3a8751f5","trusted":true},"cell_type":"markdown","source":"![](https://jgi.doe.gov/wp-content/uploads/2016/04/w1-IMG_1213_jessenmottled.jpg)\n\nThe **Cassava** plant is the second biggest source of carbohydrates for the African population. The diseases affecting it are a distressing issue that is ought to be solved through modern techniques, such as Machine Learning. Make sure to watch this inspiring [video](https://www.youtube.com/watch?v=NlpS-DhayQA) from the TensorFlow team that tackles this specific problem."},{"metadata":{"_uuid":"fe946223-16ec-4bbe-861d-6d46940f9085","_cell_guid":"99a134d1-9610-4ebd-a353-736cff370a83","trusted":true},"cell_type":"markdown","source":"## What can you find in this notebook\n- a Deep Neural Network classifier using TensorFlow\n- for some simple **EDA**, make sure to check out [my other notebook](https://www.kaggle.com/grigorelucian/simple-eda-cassava-leaf-disease)"},{"metadata":{},"cell_type":"markdown","source":"## Why use EfficientNets\n### and not juse another classic Convolutional Neural Network?\n\n![](https://1.bp.blogspot.com/-oNSfIOzO8ko/XO3BtHnUx0I/AAAAAAAAEKk/rJ2tHovGkzsyZnCbwVad-Q3ZBnwQmCFsgCEwYBhgL/s1600/image3.png)\n\nWell, the answer is simple and consists of two parts.\\\n**Firstly**, in the most common researches in the field, CNNs are overtaken by EfficientNets in every category possible. Thus, this new type of networks:\n* have less parameters\n* achieve better accuracy on 5 out of 8 widely used datasets\n* use less hardware (and have better latency)\n\n_Make sure to refer to [this article](https://arxiv.org/pdf/1905.11946.pdf)_\n\n**Secondly**, I have personally observed that classic CNNs will just bottleneck at some point on large datasets. Of course, without using 12 GPUs and 64-core CPUs.\nA previous model of mine achieved on this particular dataset no more than ~61% accuracy on the train set, which has been actually achieved before first epoch ended. The model was not able to improve this accuracy by the end of the training.\n\n_You can find that model [here](https://www.kaggle.com/grigorelucian/classic-cnn-cassava-leaf-disease)_"},{"metadata":{"_uuid":"b38a7696-9be5-4e2a-97d8-f693abd28209","_cell_guid":"f3e34678-fa80-4c42-82bd-4eb6a190c31a","trusted":true},"cell_type":"markdown","source":"## Let's get to work!"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip show tensorflow","execution_count":1,"outputs":[{"output_type":"stream","text":"Name: tensorflow\r\nVersion: 2.3.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /opt/conda/lib/python3.7/site-packages\r\nRequires: wheel, astunparse, six, google-pasta, absl-py, grpcio, opt-einsum, gast, keras-preprocessing, tensorflow-estimator, termcolor, tensorboard, protobuf, numpy, wrapt, h5py\r\nRequired-by: tensorflow-cloud, fancyimpute\r\n","name":"stdout"}]},{"metadata":{"_uuid":"a97d2cb6-18a6-4d41-8dd7-3c3f684e53d1","_cell_guid":"82177838-5543-4914-998a-cc767fc047e0","trusted":true},"cell_type":"code","source":"# libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.data import TFRecordDataset\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.activations import swish\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# constants\nIMAGE_WIDTH = IMAGE_HEIGHT = 300\nno_batches = 64\nno_classes = 5\ndecay = 0.9\nmomentum = 0.9\nbatch_norm_momentum = 0.99\nweight_decay = 1e-5\ninitial_lr = 0.256\nlr_decay = 0.97\nlr_decay_freq = 2.4 # epochs\ndropout_rate = 0.4","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# storing the tfrecords filenames\ntfrecs_path = '../input/cassava-leaf-disease-classification/train_tfrecords'\nrecords = os.listdir(tfrecs_path)\ndef _fmap(elem):\n    return tfrecs_path + '/' + elem\n\n# personally avoided using map() because it returns a <map object> that cannot be used later\nfor i in range(0, len(records)):\n    records[i] = _fmap(records[i])\nfor record in records:\n    print(record)","execution_count":4,"outputs":[{"output_type":"stream","text":"../input/cassava-leaf-disease-classification/train_tfrecords/ld_train00-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train09-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train14-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train08-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train13-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train05-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train03-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train11-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train06-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train02-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train15-1327.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train10-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train01-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train04-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train12-1338.tfrec\n../input/cassava-leaf-disease-classification/train_tfrecords/ld_train07-1338.tfrec\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # creating raw train dataset\n# raw_dataset = TFRecordDataset(records)\n# raw_dataset","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how one entry looks like (I will do it here with some lines from the first image)\n> for entry in raw_dataset.take(1):\\\n>     print(repr(entry))\n\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\n\\xc5\\xef\\x06\\n\\x1f\\n\\nimage_name\\x12\\x11\\n\\x0f\\n\\r499934842.jpg\\n\\x0f\\n\\x06target\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x8f\\xef\\x06\\n\\x05image\\x12\\x84\\xef\\x06\\n\\x80\\xef\\x06\\n\\xfc\\xee\\x06\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x01\\x01\\x01\\x01\\x02\\x01\\x01\\x01\\x02\\x02\\x02\\x02\\x02\\x04\\x03\\x02\\x02\\x02\\x02\\x05\\x04\\x04\\x03\\x04\\x06\\x05\\x06\\x06\\x06\\x05\\x06\\x06\\x06\\x07\\t\\x08\\x06\\x07\\t\\x07\\x06\\x06\\x08\\x0b\\x08\\t\\n\\n\\n\\n\\n\\x06\\x08\\x0b\\x0c\\x0b\\n\\x0c\\t\\n\\n\\n\\xff\\xdb\\x00C\\x01\\x02\\x02\\x02\\x02\\x02\\x02\\x05\\x03\\x03\\x05\\n\\x07\\x06\\x07\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xff\\xc0\\x00\\x11\\x08\\x02\\x00\\x02\\x00\\x03\\x01\\x11\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x1e\\x00\\x00\\x02\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x05\\x03\\x06\\x02\\x07\\x08\\x01\\x00\\t\\n\\xff\\xc4\\x00O\\x10\\x00\\x01\\x03\\x02\\x04\\x04\\x04\\x04\\x04\\x03\\x06\\x04\\x04\\x04\\x02\\x0b\\x01\\x02\\x03\\x04\\x05\\x11\\x00\\x06\\x12!\\x07\\x131A\\x08\"Qa\\x142q\\x81\\x15#B\\x91R\\xa1\\xb1\\t\\x16$3b\\xc1Cr\\x82\\xd14\\xe1\\xf0\\xf1\\x17%S\\x92\\xa25s\\x18&Dc\\xb2\\'\\x83\\xa3\\xc2\\xd2\\xff\\xc4\\x00\\x1c\\x01\\x00\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x04\\x01\\x02\\x05\\x00\\x06\\x07\\x08\\xff\\xc4\\x009\\x11\\x00\\x02\\x02\\x02\\x02\\x02\\x02\\x01\\x04\\x00\\x05\\x03\\x02\\x06\\x02\\x03\\x01\\x02\\x00\\x03\\x04\\x11\\x12!\\x051\\x13\"A\\x06\\x142Q#3Baq\\x07\\x15\\x81$R\\x16\\x174\\xa1\\xc1\\xf0%\\x91C\\xb1\\xe1\\xff\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\\xef\\xd4\\xf8|r:\\xd0\\xec\\xfa\\xfcB\\xa4u\\xfc\\xe4\\xd8\\x7f<|\\xc5\\xef\\x13\\xc3\\xff\\x00\\xd8\\xf0\\x7f\\xb8MK\\x87T8\\xd1\\xd3\\x19\\x9c\\xcb\\r\\xa1n\\xef$\\x8b\\xf7\\xef\\x8c\\xdb\\xef\\xb46\\xd7\\xd4\\xb2\\xf8\\xcf\\x1bWD\\xc54\\xac\\x97\\x95\\xe2\\xcaP\\x9b\\x9ei\\xae%F\\xe0%\\xd1\\xb7\\xf3\\xc2/\\xe4\\xb2\\x14\\xea2\\x98>/\\xff\\x00tx\\xceE\\xcb\\x08\\x90\\x87\\xe2q\\x02\\x9c\\x95\\x84$\\x84\\x19(\\xb0 \\xfdp\\x0e\\x18\\xc8\\xdb\\xdcg\\xf6\\x9e7\\xfb\\x93\\xd48=\\x94\\xb3J\\xd0\\x99\\\\H\\x80\\xa0\\x8d\\xd4\\x94\\xbe\\x92\\t\\xfd\\xf1\\xb7\\x8d\\x9fEi\\xf52\\x1b\\x07\\xc6\\xd85\\xb8\\xd6\\x99\\xe1\\xc3#<\\xd8z6y\\x82\\xee\\x825$<\\x91\\xb5\\xfe\\xb8c\\xfe\\xe4%\\xab\\xf0\\x9e1\\xbd\\x99t\\xa0p\\x92\\x87L\\xa6=O\\xa6\\xd7\\xe2\\xa3\\x9c\\x82\\x95\\x10\\xe87\\xf7\\xc4\\x8c\\xca-;x\\xdax\\n5\\xaa\\x1c\\x01\\x11O\\xf0\\xa1^\\x9b\\xf9\\xf034r\\x95\\x1d\\x88#\\x0fU\\x99\\x88\\xbf\\x99C\\xfaJ\\xcb\\xbb\\xf9\\x04\\xc1\\xbf\\x0c\\x9cK\\xa1\\xb6]\\x89Si\\xe2>Ts6#\\xd7\\x14\\xbb\"\\xab\\x07F$\\x7fJ\\xdb[\\x1f\\xcc\\x90\\xf0\\x97\\x89pc\\xf2\\x1a\\xa1\\x95\\xb8\\xb4\\xeaR\\xf5\\x02\\x95c\\x1e\\xfc5\\xb3\\xd4\\xb3x,\\x85B$QrF}\\xa58\\x99K\\xcbn4\\xe2Up\\xa4\\xa4\\xd8\\x9cgY\\x81r\\xfa\\x81\\xab\\xc6\\xe4\\xd3\\xf8\\x96\\xac\\xbfU\\x"},{"metadata":{},"cell_type":"markdown","source":"As we can see, one entry in these tfrecords is described through:\n* target\n* image_name\n* image\n\nWe now need to parse these entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # let's parse these records\n# features = {\n#     'target': tf.io.FixedLenFeature([],\n#                                     tf.int64,\n#                                     default_value = 0),\n#     'image_name': tf.io.FixedLenFeature([],\n#                                         tf.string,\n#                                         default_value = ''),\n#     'image': tf.io.FixedLenFeature([],\n#                                    tf.string,\n#                                    default_value = '')}\n\n# def _parse_function(proto):    \n#     return tf.io.parse_single_example(proto, features)\n\n# parsed_dataset = raw_dataset.map(_parse_function,\n#                                  num_parallel_calls = 4)\n# # parsed_dataset.repeat()\n# # parsed_dataset.batch(no_batches)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how a parsed entry looks like\n> for entry in parsed_dataset.take(1):\\\n>     print(repr(entry))\n\n> {'image': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x01\\x01\\x01\\x01\\x02\\x01\\x01\\x01\\x02\\x02\\x02\\x02\\x02\\x04\\x03\\x02\\x02\\x02\\x02\\x05\\x04\\x04\\x03\\x04\\x06\\x05\\x06\\x06\\x06\\x05\\x06\\x06\\x06\\x07\\t\\x08\\x06\\x07\\t\\x07\\x06\\x06\\x08\\x0b\\x08\\t\\n\\n\\n\\n\\n\\x06\\x08\\x0b\\x0c\\x0b\\n\\x0c\\t\\n\\n\\n\\xff\\xdb\\x00C\\x01\\x02\\x02\\x02\\x02\\x02\\x02\\x05\\x03\\x03\\x05\\n\\x07\\x06\\x07\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xff\\xc0\\x00\\x11\\x08\\x02\\x00\\x02\\x00\\x03\\x01\\x11\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x1e\\x00\\x00\\x02\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x05\\x03\\x06\\x02\\x07\\x08\\x01\\x00\\t\\n\\xff\\xc4\\x00O\\x10\\x00\\x01\\x03\\x02\\x04\\x04\\x04\\x04\\x04\\x03\\x06\\x04\\x04\\x04\\x02\\x0b\\x01\\x02\\x03\\x04\\x05\\x11\\x00\\x06\\x12!\\x07\\x131A\\x08\"Qa\\x142q\\x81\\x15#B\\x91R\\xa1\\xb1\\t\\x16$3b\\xc1Cr\\x82\\xd14\\xe1\\xf0\\xf1\\x17%S\\x92\\xa25s\\x18&Dc\\xb2\\'\\x83\\xa3\\xc2\\xd2\\xff\\xc4\\x00\\x1c\\x01\\x00\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x04\\x01\\x02\\x05\\x00\\x06\\x07\\x08\\xff\\xc4\\x009\\x11\\x00\\x02\\x02\\x02\\x02\\x02\\x02\\x01\\x04\\x00\\x05\\x03\\x02\\x06\\x02\\x03\\x01\\x02\\x00\\x03\\x04\\x11\\x12!\\x051\\x13\"A\\x06\\x142Q#3Baq\\x07\\x15\\x81$R\\x16\\x174\\xa1\\xc1\\xf0%\\x91C\\xb1\\xe1\\xff\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\\xef\\xd4\\xf8|r:\\xd0\\xec\\xfa\\xfcB\\xa4u\\xfc\\xe4\\xd8\\x7f<|\\xc5\\xef\\x13\\xc3\\xff\\x00\\xd8\\xf0\\x7f\\xb8MK\\x87T8\\xd1\\xd3\\x19\\x9c\\xcb\\r\\xa1n\\xef$\\x8b\\xf7\\xef\\x8c\\xdb\\xef\\xb46\\xd7\\xd4\\xb2\\xf8\\xcf\\x1bWD\\xc54\\xac\\x97\\x95\\xe2\\xcaP\\x9b\\x9ei\\xae%F\\xe0%\\xd1\\xb7\\xf3\\xc2/\\xe4\\xb2\\x14\\xea2\\x98>/\\xff\\x00tx\\xceE\\xcb\\x08\\x90\\x87\\xe2q\\x02\\x9c\\x95\\x84$\\x84\\x19(\\xb0 \\xfdp\\x0e\\x18\\xc8\\xdb\\xdcg\\xf6\\x9e7\\xfb\\x93\\xd48=\\x94\\xb3J\\xd0\\x99\\\\H\\x80\\xa0\\x8d\\xd4\\x94\\xbe\\x92\\t\\xfd\\xf1\\xb7\\x8d\\x9fEi\\xf52\\x1b\\x07\\xc6\\xd85\\xb8\\xd6\\x99\\xe1\\xc3#<\\xd8z6y\\x82\\xee\\x825$<\\x91\\xb5\\xfe\\xb8c\\xfe\\xe4%\\xab\\xf0\\x9e1\\xbd\\x99t\\xa0p\\x92\\x87L\\xa6=O\\xa6\\xd7\\xe2\\xa3\\x9c\\x82\\x95\\x10\\xe87\\xf7\\xc4\\x8c\\xca-;x\\xdax\\n5\\xaa\\x1c\\x01\\x11O\\xf0\\xa1^\\x9b\\xf9\\xf034r\\x95\\x1d\\x88#\\x0fU\\x99\\x88\\xbf\\x99C\\xfaJ\\xcb\\xbb\\xf9\\x04\\xc1\\xbf\\x0c\\x9cK\\xa1\\xb6]\\x89Si\\xe2>Ts6#\\xd7\\x14\\xbb\"\\xab\\x07F$\\x7fJ\\xdb[\\x1f\\xcc\\x90\\xf0\\x97\\x89pc\\xf2\\x1a\\xa1\\x95\\xb8\\xb4\\xeaR\\xf5\\x02\\x95c\\x1e\\xfc5\\xb3\\xd4\\xb3x,\\x85B$QrF}\\xa58\\x99K\\xcbn4\\xe2Up\\xa4\\xa4\\xd8\\x9cgY\\x81r\\xfa\\x81\\xab\\xc6\\xe4\\xd3\\xf8\\x96\\xac\\xbfU\\xcf\\xd1\\x0f&\\xa7\\x05\\xc7\\x12~f\\xe47\\xb7\\xf3\\xc2\\x8e\\xdeC\\x10\\xee\\xa1\\xeeiQ~v7J=\\xc7i\\xe1\\xfeI\\xce\\xcb13\\x16Xm\\x97Kw\\x1f\\x93a{\\xfa\\xfa\\xe1\\xca<\\x8eH\\x1f\\xe2\\x89\\xa0q\\xd3\\xc"},{"metadata":{},"cell_type":"markdown","source":"Now we have, for each image, a dictionary with key:value pairs for each of the features! Awesome!\\\nEvery value is a **Tensor**, which, unfortunately, cannot be fed into EfficientNets, as these are made to deal with images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# in_image_tensor = []\n# in_label_tensor = []\n# for elem in parsed_dataset.take(-1):\n# #     img = tf.io.decode_and_crop_jpeg(elem['image'],\n# #                                      crop_window = \n# #                                      channels = 3)\n#     #img = tf.image.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n# #     img = elem['image']\n#     img = tf.io.decode_jpeg(elem['image'], channels = 3)\n#     img = tf.image.resize_with_pad(img,\n#                                    target_width = IMAGE_WIDTH,\n#                                    target_height = IMAGE_HEIGHT,\n#                                    method = 'bicubic')\n#     in_image_tensor.append(img)\n#     lbl = elem['target']\n#     in_label_tensor.append(lbl)\n    \n# print(len(in_label_tensor))\n# print(len(in_image_tensor))\n# print(in_label_tensor[0])\n# print(in_image_tensor[0])\n\n# # with tf.compat.v1.Session():\n# #     print(in_image_tensor[0].numpy())","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function that parses one single \ndef _fparse(proto):\n    features = {\n        'target': tf.io.FixedLenFeature([],\n                                        tf.int64,\n                                        default_value = 0),\n        'image_name': tf.io.FixedLenFeature([],\n                                            tf.string,\n                                            default_value = ''),\n        'image': tf.io.FixedLenFeature([],\n                                       tf.string,\n                                       default_value = '')}\n    \n    parsed_entry = tf.io.parse_single_example(proto, features)\n    image = tf.io.decode_jpeg(parsed_entry['image'], channels = 3)\n    image = tf.image.resize_with_pad(image, target_width = IMAGE_WIDTH, target_height = IMAGE_HEIGHT)\n    return image, parsed_entry['target']\n\ndef gen_dataset(filenames, batch_size = no_batches):\n    dataset = TFRecordDataset(filenames)\n    dataset = dataset.map(_fparse, num_parallel_calls = 4)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size)\n    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n    image, label = iterator.get_next()\n    image = tf.reshape(image, [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n    label = tf.one_hot(label, no_classes)\n    return image, label\n\nin_image_tensor, in_label_tensor = gen_dataset(records)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing baseline network\nbase_model = EfficientNetB3(include_top = False,\n                            weights = 'imagenet',\n                            pooling = 'avg',\n                            input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3))\nfor layer in base_model.layers:\n    layer.trainable = True\n\n# building model\nclassifier = Sequential()\nclassifier.add(base_model)\nclassifier.add(Dense(units = 128, activation = 'relu'))\nclassifier.add(Dropout(dropout_rate))\nclassifier.add(Dense(no_classes, activation = 'softmax'))\n\n# let's compile the model\nclassifier.compile(optimizer = RMSprop(lr = initial_lr,\n                                      momentum = momentum,\n                                      rho = decay,\n                                      centered = True,\n                                      epsilon = weight_decay),\n                   loss = 'categorical_crossentropy',\n                   metrics = ['accuracy'])\n\nclassifier.summary()","execution_count":9,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n43941888/43941136 [==============================] - 0s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nefficientnetb3 (Functional)  (None, 1536)              10783535  \n_________________________________________________________________\ndense (Dense)                (None, 128)               196736    \n_________________________________________________________________\ndropout (Dropout)            (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 10,980,916\nTrainable params: 10,893,613\nNon-trainable params: 87,303\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _schedule(epoch, lr):\n    '''\n    We reduce learning rate by 0.97 every 2.4 epochs, as suggested in the article.\n    '''\n#     if epoch % lr_decay_freq == 0.0 and epoch > 2.4:\n#         lr -= lr_decay\n    return lr\n\nlearning_rate_scheduler = LearningRateScheduler(_schedule,\n                                                verbose = 1)\n\ncallbacks_used = [\n#     learning_rate_scheduler,\n    EarlyStopping(monitor = 'accuracy',\n                  patience = 3),\n    ModelCheckpoint(filepath = 'cassava_model.h5',\n                    monitor = 'accuracy',\n                    save_best_only = True)]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training our model\nclassifier.fit(x = in_image_tensor,\n               y = in_label_tensor,\n               epochs = 10,\n               verbose = 1,\n               callbacks = callbacks_used,\n               batch_size = no_batches,\n               steps_per_epoch = 4)","execution_count":11,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n","name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":" OOM when allocating tensor with shape[576] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/efficientnetb3/block5a_bn/FusedBatchNormV3 (defined at <ipython-input-11-281c32cf948e>:8) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_37604]\n\nFunction call stack:\ntrain_function\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-281c32cf948e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_used\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                steps_per_epoch = 4)\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[576] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/efficientnetb3/block5a_bn/FusedBatchNormV3 (defined at <ipython-input-11-281c32cf948e>:8) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_37604]\n\nFunction call stack:\ntrain_function\n"]}]},{"metadata":{},"cell_type":"markdown","source":"*References:*\\\nhttp://digital-thinking.de/tensorflow-vs-keras-or-how-to-speed-up-your-training-for-image-data-sets-by-factor-10/\nhttps://arxiv.org/pdf/1905.11946.pdf"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}